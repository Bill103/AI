{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler           #import the necessary modules \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "from keras.preprocessing import image    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/vasil/Desktop/tensorflow/FER/data/icml_face_data.csv')\n",
    "pixel_data = data[' pixels']\n",
    "label_data = data['emotion']            #open the csv file and categorize the data to pictures and emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pixels(pixel_data):\n",
    "    images=[]\n",
    "    for i in range(len(pixel_data)):\n",
    "        img = np.fromstring(pixel_data[i], dtype='int', sep=' ')    #define the function to iterate and reshape the images to a 48x48x1 format\n",
    "        img = img.reshape(48,48,1)\n",
    "        images.append(img)\n",
    "    X = np.array(images)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy='auto')\n",
    "X_over, Y_over = oversampler.fit_resample(pixel_data.values.reshape(-1,1), label_data)  #oversampling magic to equalize the dataset\n",
    "X_over_series = pd.Series(X_over.flatten()) #flatten the dataframe of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_pixels(X_over_series)\n",
    "Y = Y_over                                              #use the fuction from earlier to the oversampled images\n",
    "Y = Y_over.values.reshape(Y.shape[0],1)                 #reshape the values of the emotions                                 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 45) #split the dataset to training and testing batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_5452/2285503216.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m              \u001b[1;31m#plot the first picture of the dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "plt.imshow(X[1,:,:,0])              #plot the first picture of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', strides=(1,1), input_shape=(48,48,1)),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', strides=(1,1), padding='same'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', strides=(1,1), padding ='valid' ),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(400, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46, 46, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 21, 21, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 400)               819600    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 400)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 2807      \n",
      "=================================================================\n",
      "Total params: 1,101,255\n",
      "Trainable params: 1,100,423\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "y_train=tf.keras.utils.to_categorical(Y_train, num_classes = 7)\n",
    "y_test=tf.keras.utils.to_categorical(Y_test, num_classes = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "1770/1770 [==============================] - 54s 25ms/step - loss: 1.7589 - accuracy: 0.3479 - val_loss: 1.3518 - val_accuracy: 0.4971\n",
      "Epoch 2/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 1.3089 - accuracy: 0.5008 - val_loss: 1.1215 - val_accuracy: 0.5778\n",
      "Epoch 3/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 1.1102 - accuracy: 0.5774 - val_loss: 1.0058 - val_accuracy: 0.6204\n",
      "Epoch 4/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.9714 - accuracy: 0.6333 - val_loss: 0.9099 - val_accuracy: 0.6630\n",
      "Epoch 5/35\n",
      "1770/1770 [==============================] - 43s 24ms/step - loss: 0.8569 - accuracy: 0.6800 - val_loss: 0.8576 - val_accuracy: 0.6904\n",
      "Epoch 6/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.7519 - accuracy: 0.7206 - val_loss: 0.7861 - val_accuracy: 0.7114\n",
      "Epoch 7/35\n",
      "1770/1770 [==============================] - 43s 24ms/step - loss: 0.6556 - accuracy: 0.7609 - val_loss: 0.7284 - val_accuracy: 0.7435\n",
      "Epoch 8/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.5769 - accuracy: 0.7902 - val_loss: 0.6980 - val_accuracy: 0.7620\n",
      "Epoch 9/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.5036 - accuracy: 0.8176 - val_loss: 0.6558 - val_accuracy: 0.7834\n",
      "Epoch 10/35\n",
      "1770/1770 [==============================] - 43s 25ms/step - loss: 0.4417 - accuracy: 0.8414 - val_loss: 0.6461 - val_accuracy: 0.7877\n",
      "Epoch 11/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.3849 - accuracy: 0.8607 - val_loss: 0.6370 - val_accuracy: 0.7988\n",
      "Epoch 12/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.3411 - accuracy: 0.8796 - val_loss: 0.6240 - val_accuracy: 0.8093\n",
      "Epoch 13/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.3030 - accuracy: 0.8931 - val_loss: 0.6388 - val_accuracy: 0.8139\n",
      "Epoch 14/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.2692 - accuracy: 0.9040 - val_loss: 0.6163 - val_accuracy: 0.8279\n",
      "Epoch 15/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.2403 - accuracy: 0.9149 - val_loss: 0.6337 - val_accuracy: 0.8339\n",
      "Epoch 16/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.2163 - accuracy: 0.9239 - val_loss: 0.6416 - val_accuracy: 0.8314\n",
      "Epoch 17/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1998 - accuracy: 0.9296 - val_loss: 0.6578 - val_accuracy: 0.8320\n",
      "Epoch 18/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1802 - accuracy: 0.9377 - val_loss: 0.6928 - val_accuracy: 0.8304\n",
      "Epoch 19/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1694 - accuracy: 0.9395 - val_loss: 0.7080 - val_accuracy: 0.8338\n",
      "Epoch 20/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1575 - accuracy: 0.9457 - val_loss: 0.7268 - val_accuracy: 0.8327\n",
      "Epoch 21/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1507 - accuracy: 0.9477 - val_loss: 0.7176 - val_accuracy: 0.8393\n",
      "Epoch 22/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1411 - accuracy: 0.9497 - val_loss: 0.7552 - val_accuracy: 0.8357.1407 - \n",
      "Epoch 23/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1318 - accuracy: 0.9543 - val_loss: 0.7436 - val_accuracy: 0.8403\n",
      "Epoch 24/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1312 - accuracy: 0.9551 - val_loss: 0.7696 - val_accuracy: 0.8387\n",
      "Epoch 25/35\n",
      "1770/1770 [==============================] - 46s 26ms/step - loss: 0.1179 - accuracy: 0.9582 - val_loss: 0.7671 - val_accuracy: 0.8401\n",
      "Epoch 26/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1124 - accuracy: 0.9612 - val_loss: 0.7717 - val_accuracy: 0.8341\n",
      "Epoch 27/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1095 - accuracy: 0.9629 - val_loss: 0.8389 - val_accuracy: 0.8420\n",
      "Epoch 28/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1037 - accuracy: 0.9648 - val_loss: 0.8888 - val_accuracy: 0.8366\n",
      "Epoch 29/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.1003 - accuracy: 0.9655 - val_loss: 0.8659 - val_accuracy: 0.8398\n",
      "Epoch 30/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.1021 - accuracy: 0.9648 - val_loss: 0.8499 - val_accuracy: 0.8344\n",
      "Epoch 31/35\n",
      "1770/1770 [==============================] - 45s 26ms/step - loss: 0.0980 - accuracy: 0.9664 - val_loss: 0.8695 - val_accuracy: 0.8424\n",
      "Epoch 32/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.0915 - accuracy: 0.9682 - val_loss: 0.8858 - val_accuracy: 0.8428\n",
      "Epoch 33/35\n",
      "1770/1770 [==============================] - 44s 25ms/step - loss: 0.0889 - accuracy: 0.9698 - val_loss: 0.9401 - val_accuracy: 0.8430\n",
      "Epoch 34/35\n",
      "1770/1770 [==============================] - 45s 25ms/step - loss: 0.0883 - accuracy: 0.9700 - val_loss: 0.9440 - val_accuracy: 0.8347racy: \n",
      "Epoch 35/35\n",
      "1770/1770 [==============================] - 45s 26ms/step - loss: 0.0869 - accuracy: 0.9699 - val_loss: 0.9233 - val_accuracy: 0.8381\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x164766af5b0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=35, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('model')\n",
    "#preds=model.predict(X_train)\n",
    "#def get_class(preds):\n",
    "#    pred_class=np.zeros((preds.shape[0],1))\n",
    "    \n",
    "#    for i in range(len(preds)):\n",
    " #       pred_class[i] = np.argmax(preds[i])\n",
    "\n",
    " #   return pred_class\n",
    "#pred_class_train=get_class(preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted result is: Happiness\n",
      "[[1.4758285e-08 2.6344340e-19 1.7603906e-10 9.9994707e-01 8.5003053e-09\n",
      "  2.2061487e-12 5.2941123e-05]]\n"
     ]
    }
   ],
   "source": [
    "label_dict = {0 : 'Angry', 1 : 'Disgust', 2 : 'Fear', 3 : 'Happiness', 4 : 'Sad', 5 : 'Surprise', 6 : 'Neutral'}\n",
    "img_path = 'alibaba.jpg'\n",
    "img= image.load_img(img_path, color_mode='grayscale', target_size=(48,48))\n",
    "x= image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "prediction = np.argmax(model.predict(x))\n",
    "print('The predicted result is: ' + label_dict[prediction])\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a09ddbbea9b09bd8fe9d9c3d3756891efc55aae894e87570a5b525b82263952"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
