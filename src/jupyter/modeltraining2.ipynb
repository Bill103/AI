{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import RandomOverSampler           #import the necessary modules \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf \n",
    "from keras.preprocessing import image  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('C:/Users/vasil/Desktop/tensorflow/FER/data/icml_face_data.csv')\n",
    "pixel_data = data[' pixels']\n",
    "label_data = data['emotion']            #open the csv file and categorize the data to pictures and emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_pixels(pixel_data):\n",
    "    images=[]\n",
    "    for i in range(len(pixel_data)):\n",
    "        img = np.fromstring(pixel_data[i], dtype='int', sep=' ')    #define the function to iterate and reshape the images to a 48x48x1 format\n",
    "        img = img.reshape(48,48,1)\n",
    "        images.append(img)\n",
    "    X = np.array(images)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "oversampler = RandomOverSampler(sampling_strategy='auto')\n",
    "X_over, Y_over = oversampler.fit_resample(pixel_data.values.reshape(-1,1), label_data)  #oversampling magic to equalize the dataset\n",
    "X_over_series = pd.Series(X_over.flatten()) #flatten the dataframe of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = preprocess_pixels(X_over_series)\n",
    "Y = Y_over                                              #use the fuction from earlier to the oversampled images\n",
    "Y = Y_over.values.reshape(Y.shape[0],1)                 #reshape the values of the emotions                                 \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.1, random_state = 45) #split the dataset to training and testing batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a6381bcdf0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD6CAYAAABnLjEDAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAvV0lEQVR4nO2deYxd93Xfv+e++/ZZ3uwccriIWinJsZQosmQ7jSFbqeykVlogTZy6kRsX7h8t4KApYrkF2gZICxUBkqBwClit3SjNBrcOKtlw4sqKbdmOJJuSaFmULJOiuHP2eTPz5s3b7vv1D44UnoWckSg9jnzPBxBGvzvn3vu7y++9OV+ehUIIcBznx5/oSk/AcZze4IvdcVKCL3bHSQm+2B0nJfhid5yU4IvdcVLCZS12IrqHiF4ioqNEdP+bNSnHcd586I3+OzsRZQD8CMDdAE4D+B6Aj4QQXrjYPtnBYijsGOQbxem7ILVfTF02ToK26SQZfti2/hyLcgkb9+eayiYjzhVTomyW20W1TZKN9H7ZiB87g66ysc4niYjfNPsR8ntkHZfEzW+FWNkE43lEYr+O8Z3RCfx5dLraRj7rZqLPnyR8v0p+XdmsJTk2bjWyyqZcaqhtEuv8nY6Yd0tfB8nHWNTPtZxrsXFtPb/pfBDpBxuJbV0xv87CEpLVNf3QAOir2zq3AzgaQjgGAET0FwDuBXDRxV7YMYif/G8fZdtCkA+cvyQAMFTgD3ilWVA2s9U+Nk6mS8qmvG+Zjd83dVTZ9GX4B8B4bkXZ/PXMTfxcQb8Ak6Vlva3AjzWY0S/uaHZVbZOUIz7HdtD3TM5pLNbXIT/YzrSHlU2zqxdOSZx/MSkrm/l2P7dpaZvVDn/hT6wMKZuFZb7fvdf9QNkcXNjDj/PSDmVz+61H1DbJ8WV9/QvivcJJ/UEfr/N3OLlhTZ9/7wk2/s6L1+gJdPhxonJHmZT7+IfW6iyf3/Tv/Fd93FePd9HfbM4uAKcuGJ/e2OY4zjbkcha79aeC+ruDiD5BRAeJ6GC7Wr+M0zmOczlczmI/DWD3BeMpAGelUQjhwRDCbSGE27IV/ae14zi94XJ89u8BuJaIrgJwBsCvAPjV13uQrvDZpYgFAO0u90mX1rTf1Fng22hUi2/vmjy56XxGszU2fmZlj7KZW+N+5K4B7Q8vNrWPOhDzOQ3H2reTWD6zJexJpB/fCMZxhLInnwWg/XMA6BdaQ72rxaasEAQtwdI6n5pjhs/xS0dvVjY/u49rL9MTA8rm4JPXsfFVt5xRNu8c1dtezHD/fyHfVjaNU1yfKDynn/33Iv4e3XnDy8rmiRe4H99taC2GhIRQGuF/LUfxxd+NN7zYQwgdIvpXAL4KIAPg8yGEw2/0eI7jvLVczjc7QghfAfCVN2kujuO8hXgEneOkhMv6Zn8zyAgfvZxtKZtzq9wnqs9onwhF7hPesue0MmkK33/MCKo5tj7Gxt89tVfZ9Jf4flbAiNQZAGBF/LvymWZF2dS7PEAkT/rfWhc7/PqlfwwAO3NLbFw1/i28lvB4hUhFhwBtI9BmOeFCa8PQFVY6/Ni1Tk7ZtEQQS4Z0EElfkd/rpWV9HY+f4L7uh6/R/xb/xdatbHzq27uVzdmbta//3t3H2PhcYVDZHBPSQz3Tp2yKh/i257I7lc0Hb+Hz/uq3blE2K8Svf2KyysYy6Ib97qK/cRznxwpf7I6TEnyxO05K8MXuOCnhigt0MoPLCrRYWuLiBiXa5pZreKLBWKGmbPIRF7ukGAYA3zl7FRsnHS20NcU2a87W+ftiLj52jQSahRa/VinqWcfpj3VGV7PJH20p0sKnTJZpGmKcJb7J62129X6zTS6qVps6EEqKmJbQSeL9yBf0ddSX+bG/+MKtyuYXbuDi18PNW5RN9kdaoPt6+1o2vmPvcWUzMcDfoxllAax3+P0oPWEIfZURNt51kz7SzPd4kM/aMD93t3vxQCX/ZneclOCL3XFSgi92x0kJPfXZCQGxCKKRftor89xvAQAsc79xcI8uDKF2MarJ7C/Ns/Hfzu3X+y3zgJF8USc+bIXYSPxYafNAk3Ksg3oqWZ5kUsxoH1Umuawn2q+uiuu3AlbKolCHDIQBgNW23iYrzFiaRU5c/2hBJ/3MN3iAiHUcqelkMjrwJ1vi96hd01rMX798gI1/7madxvFofEBti4/xOX67qYtOvPc6nohTb+vn0drBl1qnqoODTn2NB3C9+97vK5vTQ+Ns3D7NtYBuS2tMr+Lf7I6TEnyxO05K8MXuOCnBF7vjpISeCnQRBZRFQMipFR5c0KhqQSg3zkWr0T4t9kghaTKrRbzjdS7+nZmv6DlmNi+tXcpx0U6KSACw0tICYUsEkUjBDgDWjCAaSa3NbaxsNUkuowXD9QwXkuYaOlvLKq+cz/DgpFykM/OkQGfR6PDzF2J9HOvebkY2q89dX+LP47kFnXV23ZQOYnmpxmuo5o/rZ/Zkfh8bW4E3jRa/1uoeLeJVnuHC4teeuUnZ7NjPRebqkxNsbCRJvoZ/sztOSvDF7jgpwRe746SEnvrsnW6ExQYPWpFJLlFROx17RnnVlfmaDki4ZYJXBh3M6m4rXz3GgyY6VR18EQ1wf7zd0rdoLeb71VuG/1XUySl54ZMuNbRfPxv4/bD8WEnJqO6zLvzhYqyDg2rEfX/LP99KoMtgbvPWSmtGpRrJQH7z45jJMmIsW0YBQH6ABxCpTi8AUDHmtJN36Kk1dQJL9iV+rMNl3ZFGvp+HjH4qa7t4R5zxJ3SATOEa/j40dvBxyHqlGsdJPb7YHScl+GJ3nJTgi91xUkJvBbokg7kVIYyIyhqT41W13/QKz+zZMaDbGo/n+bYjq2PKprGweV/17pq4JbEWPBrExabIyMRqZLWw1ujwY681tGiVi3lASDGrhTXZV97qvS6rwFhCm7RpG+2yh41stalSlY2twBeZibcGfa3jJf7MzB7uopqOFDkBYBVcaBwoa6FPtQY3KhBNn9Qtm0d38eCszKRuTpq0uWC8+gOduXn8p/kzK+e1qLq6l8+7e1QH8Bw/xrPedl3Fg2wW8hcXdP2b3XFSgi92x0kJvtgdJyX01GcPAWiJqqdDI5v7bS0R2DJVriobWc308LlJPQGZ5GIFIAgNgYzEGFnBMzaOUzJ8ben/loa0jWxtbFWhqYsAlYZhU2uJNlJGIoy81/MrOlipb1xX05E+esdodVXM8GvrZrUf3TGq60qWmjwIy9IHiiIxSVakBfT1142W2ur9ALDwMvfjR65eVDbzosJr8YTWJ04e5u/jO259RdmsD/D3vL5Da0zDz3Cbgev4fbUqEr2Kf7M7Tkrwxe44KcEXu+OkBF/sjpMSeltKmoBYVBGRQSTzS1xoA4Crd8zxfYzKKMdWR9k4OVVSNjQqhJxYB8PI+cWxIWwZARmSgpFltqPIxchKrAM0ZEsqKXQBQCwq01SNqjgyW04GlQBaoKv06UzByeKK2iaxSmIXIj7vIeNaby6eZuPVrg4iOdrglVgGjePIwJvljr4f50S22vK6Plen32iRtcyXyOJRHXhT2s2fa3NZL6vSGT7HU1fr7LnJfn6cw1MVZdN/gh/n6Ax/75udiy9p/2Z3nJTgi91xUsKmi52IPk9Es0T0/AXbhonoUSI6svFz6FLHcBznyrMVn/2PAHwGwB9fsO1+AI+FEB4govs3xp/a7EBR1EVfiQcBzC7wNrnFkvabhvPaT5OcmOW+VFLW/jhFIuDAiD/I57mvaSUsJMLXHS3pZJGC4WvLAJmYdCVZ2VZ6rattZJVaKxCpLKrXlGKj1fEWqsdYFWePd/i9PjA4rWxuEv74vty8stkX87bWVaP18wdKx9h4KtbzaQeuqzytJQQ8sszbOA9OaX3iqdm9atvsOL/X+R9qPaAxKhKjJo1EnFmuIa0e1skye9/NKzLlR/UcW/38+pNpftzQvvj396bf7CGExwHIsKF7ATy08f8PAfjFzY7jOM6V5Y367BMhhHMAsPFzfBN7x3GuMG+5QEdEnyCig0R0sLO8+Z/jjuO8NbzRxT5DRJMAsPFz9mKGIYQHQwi3hRBuiwf1v307jtMb3mhQzSMA7gPwwMbPh7eyU7cbYXWNBzOEhAd73DiuxR7ZJqlhlDxuV7mQla1olUaWGI6kYAcgL4JodvdXlY1sYWWVYJY2gO61vmr0Q5/egmgmBbnEyB6rN/lxai0t9MnsMKtSzZmTWkjKiuCTj039rbJZ7HAh6am6DiL5esLntDevRbwb87wE8/dbxj1rV9h4OdFfKjVxrqYhBlqBR901LqquTxmtrkRLqNxNuvXY2k5uM/SCPteR63l1JaviznofDzorzPLjRFoX/rvfXfxX5yGiPwfwBIDrieg0EX0c5xf53UR0BMDdG2PHcbYxm36zhxA+cpFfvf9NnovjOG8hHkHnOCmht5VqEkJ7hftO+67i2p4V6DGc50Er33n5amUTNYU/blR8LRS5r2kmhwi/vmX447Id8a5iVdn8dN8xtW0g4j7YQqIDRL6zci0bn1zTwYkrTe7/rRntp2SL4IxxP9Q+6/reU8sI2ClxPeS/n/gZZXNqls87YyUUNfgcSwPaR40iPm9rjp0FoQOV9LniAve1OyvGe7arqrZRQRzLqARTeIm/I83nKsqm/508YIae1zZrp7k/Pn7gnLKpCckizw8Lo9Dwa/g3u+OkBF/sjpMSfLE7Tkrwxe44KaG3lWoyAblBLu7IVkZVo2e5rJaSO2JkHu3l4htp7U2V2d1VqSqbsQLPxLqhrIN8pnI8L2h3dkGfzOBIU/ftltzRf5SNT9dvUzanp7n4FYx+5BDCWtRnRFvIJEBDtMrP6UCbepWfv7Wmb3ZZxBSR0ZWosMQnkOQN8a3Ij102BKjCEhfxoraec5Lnr3qS03OmRAcQlce5XWP04qWaXzuOoYXKTMnOqD5/YZbPO7pRn6s9wLcV55TJRfFvdsdJCb7YHScl+GJ3nJTgi91xUkJPBbpc3MGeUR7ys7jOM5SurWjF4fHjPGKuuKRM0L2RK0Iy8goA+gtcDLxpUEcoHSieZePhTE3Z7Ih5VtOhhi5n9Fxtt9r2rTP72bi2qjO4dk/wi1tYM9KCV0TEXM5QhLJim6ErhWVRtvqs0bNtRu+YqwUx1urb2jh/tTplLUg1K4aKqibJh9TV82kO8uPkVpWJojFszMcQ36a+xt+Z5f1aROz+PBds42/pctNrZ0W05C6tNFaeFyWxG0a56yF+r6M2v8+XaPXm3+yOkxZ8sTtOSvDF7jgpocf92UlVQ1lt8Cw4q91R+Ru8l3bTqFKfiJZMVpWPn9/5PBu/q/Sysrk+ywN4vtXYpWw+M81T+X+0pOttzi3oNlbdBr/dpWM6W21WlCq2AjRkSNG6niIyS/xzvJvTj1oGHsW6IjbKM0YUi9hv4SZ9HR0hNVjZWEmOO5hRov3ojKimnBS0TXuA36SkqG+azIoUj/n8cYa19nD273EfvfKSdopnZvizrugK0Mis8fOHKf1+xqIl1dKy7iEfiew9klmZ7rM7juOL3XFSgi92x0kJvtgdJyX0VKDrdCPMrXLRYayfq0JfP87LMgHA5BEeMHPq54zAhg4Xbg6M6Gy1w7VJNv7r6RuVjSxVdWa+omySJS4qxqv6M7O4pIUkVfF5CzElVoBI31mudmUOaUGqMcIPvrpbz3F9ih9HZqEBQOm4VrJOfpgHjXSKer+u0OxCbJRzmpMiojJBc5jv180b5yrw66e2URI63xVjbZNd1MuhuYMLYqVv6/P3/5BPfOUa/Ty6Il0vZ5TpknSq+oYUxrj6l4i4G6Oq+Gv4N7vjpARf7I6TEnyxO05K6H1QjagiMtVXZeOlr+xU+3VFz3QraAI17iQ+8c2blEkifMuRq2UnauDaIZ6IM1/TgQ3NVR7WkmkYPqKOM0EQeSaR7hCl3Pj6hPYR16b4Z/TEU/o4Q4eqbFya1tdB3+XjwgtnlE3ral1dpyWqpcTrhj6R4TbZJf29ovx6nYeD8hl+7KyhYbQG+WvcGNH3TCaIdMraxgzGKXKffXVKJ6c0xoSuUNDHkSWp83kdPNYa4Ncar+obEk/y48hKPkZ19Nfwb3bHSQm+2B0nJfhid5yU4IvdcVJCb0tJU0ChwIWJk6s8hW3Xl3X1mNZubhOv6WnvfJwLKcv7tbjxK//ia2w8ZKR5/c/jd7Lx+kmdvRbKXIDpDGqxxxJXclWunlhBJN2sEL/qhvgn4jEWbtTnGniGX1v+2EllE43x0sndmr4fmaYO/siLgKG+01qQChluk2lqGylIWb3Fh58XipxRI3zhHbwKTDc2qtCI4JykrOeT6dcTGBzg96Q1oMuYd/r5PaKivmeRECwrRZ31Ni9b2G9etfp1BWr5N7vjpARf7I6TEnyxO05K6KnPnom6GBAVXs+KRJPh9+ighXXRgseqqBLXhZ90t46+WOzwwBIrEWZ2foCNuwO6ekk8z6NBLL86KVjlXPnQCqqRgSZJXtvI6jWdfn2u0MdLxTQO6OCYE78q7lkYUzbXfFb7nzJAZX1Uf2fkl0WgifGmyeo11rU2JriPXB/TB1qbFC2axrU/Xt7DE3p2lHQ5mWxGX2t/lr+vL47qFlFZkZzSaWkNJRLHHi3qqsXn+jfXayRWINLF8G92x0kJvtgdJyX4YneclLDpYiei3UT0dSJ6kYgOE9EnN7YPE9GjRHRk46dR89VxnO3CVgS6DoDfDCE8Q0T9AJ4mokcBfAzAYyGEB4jofgD3A/jUpQ4UUUAxywMXuos8siTTMrK8buYiSWRUPVk7ytWd9aYOkHjk6DvYuLmuU9OC6GtePK4jX/pO8fMXlrWIl10xhC3ZusjQX+rj/HwN3UkI7T6+Y0fHeaA9zAW6E/9Af65//j2fY+MbjbI4d1R/U20bfYZfx9ydRhBJnZ/PKoktg4xiowTz4vX8GVn90WUVGhppKpuRcp2NK3l9so5R5uXUCo90sbIZiyKDrWP0hw+iP/tIvq5skj5+HYU5fZxuVwQiyXfocoJqQgjnQgjPbPz/KoAXAewCcC+AhzbMHgLwi5sdy3GcK8fr8tmJaB+AWwE8BWAihHAOOP+BAEB3Sji/zyeI6CARHWwtGx/djuP0hC0vdiLqA/BFAL8RQjD6adiEEB4MIdwWQrgtN2j8vek4Tk/YUlANEWVxfqH/aQjhLzc2zxDRZAjhHBFNApjd/DhAPsP922yVf94MP8lbJgPAyj7e36hlJJ4U53iEyvSC/mDJj4i/LJa1A1Z5SbSROq798fl38NtW26Nv444njASS07zVc3N3RdnEImEkZ1SulUFG2VWjJVIfn9PttxxRNu/M8cAOq97pf/z7/0dt+91T/5iN8zP6+rvX82N3E+1/rg+LbUb7JxnBUxg0/PE+7v+2OvpchZj71cstHbx1++gJte3wMf7uUZ8RsJPn795azWi1LFp/Nbt6jvlxfh2dWZ2E1axxbUrl81xO+yciIgCfA/BiCOH3LvjVIwDu2/j/+wA8vNmxHMe5cmzlm/09AP4pgB8Q0aGNbf8WwAMAvkBEHwdwEsAvvSUzdBznTWHTxR5C+DYuLui//yLbHcfZZngEneOkhN5WqkFALuKCV6YlAkQmZLkO3eKmM6ilpKUbRJ93XRUZrQF+ucWzRnaSCOpZ2atvURCbMrroCBqjWvxrjPCsMlmpBQBI6IH1SaPqyii//oEj+jpW9vFJ/vrIc8omS+KzPmjx6brcjNqW+9l5Ni79b50JNrtDCEljOohkdIynL5JMpzMYMoJRSkJ8q7aMajIiqGWypP9B6a9OHFDbMqIlVHavUc1HzDub06Jua1m0iDLmODHIg5pOTGibaEkEeXmlGsdxJL7YHScl+GJ3nJTQU589ooBcRrav4f5Oc1iXK2n3c18yalqBJvw4/a/o8yd57vxLLQAAqjfIcjJGFRiRiJOp6/l0SsbnqPCnuhnj2DLOxGglFET10mxNn2vxJ0Q104z2dSV9kVEqxuBj+3m/qd9/1z3KpvIDrllUf1L7n/tHFth4srisbPJC45FjAKi2edJPJ6vvx4kVnpT5o9MTyibU9XLI7eb3LZPRz2O1we9b3vDZmzHfb6bep2yGCjzoK67oUkaZo/w+dvile8tmx3F8sTtOavDF7jgpwRe746SEHgfVALEsWSI+bmq7jAyqPi540LoOImmN8OO2540auyL4oTmuhZTcAt/PqkySDHPhJL9TZ2K1jHLCsnl2oagFmCThN6S9WFI2WVHKOtPUQt/+d/CoovGMrkITiZsfGREZg5ERMST40O2H1Lav9N/ExpWntBp69OX9bHx4TItf6tlnjd7nhoiqjfgwV9CVjAZG9D1qtvn7mHQNcbjBn0cuZ1QpEqJqraHF0J8aPc3GP+zo8t9FUd1HCnSXlfXmOM6PB77YHScl+GJ3nJTgi91xUkJPBboAXa5XRvw0h4y0HSHAZJrappPjws3KdVokKZ3iollrp9EzXJRuzqzrz0Mp7oz26UwoK4OrnOWCXCWnC3B2hYj3xOI1ymboBT6eu02Z4IOjL7NxZNRyrgdR1hvaZrlrRDSKML/FlhYRb9g9zcYvrk0pm/wsf/0yDePZiyZxIW+UkhbRabHRZ71U4iJqKa/F0Wykr18KdH0FLcbWIy7Qra1qMVL2Z7feD/mM4tP63suMS9kv71KJg/7N7jgpwRe746QEX+yOkxKueFBNV2SQNXTRExUQYbUSItG2KZ7QWV71wDOGCid0a6fkBu5/5ye1/3dgjFdvGcrpc2WNSZ6uV9j45Kpuj3fqFV7NZuRpHZxT28VvSGZSawZzLV6G+EhLB2jUu0tsPJbRx1noltW2RDyQXcWqsinH3Cc+OaGvtZ6IzC+rlLTwdYMRVCMDSRIjoKmR4X615doOGz3b81ke1CN9eAAoCJvMgD5OLLLlBgo6WKna5u9nu6J1p0xTVM6Rbd7dZ3ccxxe746QEX+yOkxJ8sTtOSuipQJcEQr0jhJIsVxSMpCJdHtfQaCQZo+QTjXPhpBkZpXrP8G21Ph3Y8Ow6F/byRgbVel2LfzTDj5Vb0hc7NC8Ey2EtWq3v4Dcgb1yrDNBY7OgySDNtXra7YaT4FSJ9bfWEX0dfRgeajOd4qeaJ/bp081+s82igMKvvdVeU5aKcfviREO2sgBW5rWu8aMvrOhhGCnSNlr5HqzX+zowObZ49JwOsAGAg5vcxP6qFvug0F16bQ0LANJItX9v34r9yHOfHCV/sjpMSfLE7Tkrorc/ejbDYEEEasgORlUBS4b5LY9GYtvBbEyNAoyMqisiSzAAQjYgqNFlt0xD++NqcTgSJl/UcZdJCN6evtTHK5y0THwCgfy8vuXz96Kyy+d7cHjY+FOlElEjc63pb+6ODRvDHSIEH38gyzQAQRELPB3a+pGxu33ecjZ9cuk7ZUIM7oVap5G4kk0y0DcCP027pG9s2nvVWSJr82GtNrdfIe21RyfLgrDg2EnMq/DiT7+QJR+fKWmN5bQ6bzsBxnB8LfLE7Tkrwxe44KcEXu+OkhJ4KdN1ASgQKhgghkZVgTkX92kiIMlZQTSLPZYh4QQVbGGWBhQm1t/aZKQW5xGitFovEs/awvj9XV6ps/MKszmhbm+ZCaG5EC2050ZNsva4n1CjrV+ToUX6+PV/W9zGu8/v2hbvfq2zy13OhMTOsg3PoOA9YiQzhszkmzl/SJcKlsNdtGuXIjWy5IF6jrCHixYXN7+PgAH+whYwW0rJCwa0YWXij75pn4/eNcuHzhJGB+Sr+ze44KcEXu+OkhE0XOxEViOi7RPR9IjpMRL+9sX2YiB4loiMbP/U/tjqOs23Yis/eBHBXCKFGRFkA3yaivwLwjwA8FkJ4gIjuB3A/gE9d8khkBBdI39rwf3eUeRLFidK4PrZwbTtt7X/Fwt9KyPLRuP9XX9bJMsrXj7U+0Ok3AjRk8EdDn19WWE3WtT98ZIZXs+n7G11NZvd3eBWa2Xfrz+LF27nfGBX0nGvG9cve68Wzuq96p5/7rcPP63s0Hw+wcTKkfW3Vaj3o+1GY4a9xc0TbqD73llRkfPW1RQBVKBh6gJhTMI69a4C/wwf6p5WNTDr6tb1PKpsc8fPrBCczogjAFr7Zw3leLX6T3fgvALgXwEMb2x8C8IubHctxnCvHlnx2IsoQ0SEAswAeDSE8BWAihHAOADZ+Gl+3juNsF7a02EMISQjhFgBTAG4nopu3egIi+gQRHSSig53li/+zgOM4by2vS40PIVQBfAPAPQBmiGgSADZ+6myM8/s8GEK4LYRwWzyoE0Ycx+kNmwp0RDQGoB1CqBJREcAHAPwXAI8AuA/AAxs/H97sWNkowY4yr+IxE1XYOGrqz5/RHA9IiAd0lY9OlQspnXV9aVGeC1BWdpQMooiMXtuyXG9I9JxDVx+c6lyQK5/W+3Vle5+OEbDyfS7KDL6sg1EQ82OPPStrDgMgfpylm7VgGBmttpoVPp5+96CykbQqelsyyAWpuKjFr84kV7uCIWoWT3LBMLus72v74rrV3x3bqm4kSpR3jOchq+fIyjkAUBHBLqVIv8OTWS6qDmR0INTx1qg4Dn/20SXKOG1FjZ8E8BARZXD+L4EvhBC+TERPAPgCEX0cwEkAv7SFYzmOc4XYdLGHEJ4DcKuxfQHA+9+KSTmO8+bjEXSOkxJ6mgjTHzdU4P6xJd4jeX2+ovaTlVKt6p3TNdFr2fCZt4L0ta1KpaodVUb7SSHRvmVukX+29p3W+y0d4AfvDGjNIClzm+k7deIFdfg2o0is2jZwRM95bae+/vWdouLqtdrX7h/kSRzBaJu0q1+3m5Kcm+d6gOVXd8qbt/SOrHbQgqSsjx1EoBFZlXyFj14saQ0ls4VKNXXRHtvy2QvEH1pblDIKlxNU4zjOjwe+2B0nJfhid5yU4IvdcVJCTwW6HCXYnV1k2+7ceZyN/9+MjsQtiqoeEyUdIFId4tF5jQWdrSWDaEzxTVSq6RoBM+q4VjDGuha78gt8Am0joDC5hgtbOwwxUoo91V36WmXVlfaSbm2UXeI2uWWj/LbRI1wGNQ306zDoAyM8oLIca9GqJSKIEiOjrSOex+KyzvBri6yMblW/1tlVERxTNJ6ZJeqK6kb5klY6o4jblPM6YEa+w1J0BoDlhL8Q+3LzykZmxrWTrS9h/2Z3nJTgi91xUoIvdsdJCT2vLrvW5Qkrt/e/wsZPTuxT+720MsHGQ3ntI1b6+LbpOctnF+1tDR9RJbBYlWNlIkyk/T/pIwJAbpXbLV+jDy0DMgbzOrAiEaVS1zK63VBbVOEho9WVTA5pb7GwmEw6WqzptlHPitbGI336me3p54kfkwVd8WalzJ9jYrRarosquTSsTLA2w319meAC6AAaAIiEHhPH2kb66ONlrSlJPWK5owUbmRyTJR2sJBNfZJttki/nBfg3u+OkBF/sjpMSfLE7Tkrwxe44KaGnAh1RUEEBsrDGHSLIBgBOrXHlaDKvhZz1AS5ULI7o4ItWlWcVmW2b8pu3o5LVY6zjZFe0+Le2UxznaqN6jKCS1y2AxvM80ObJxj5l027wRxstaBEtkRl1RhWWzJq+tqTE71FhSIuI/UUuJK02dGbeobVdbHy6r6JsukLYijNGFqDoo1XMGyl+snqMEUBTGdXPo7/Ar2Pd6GEvRcM+I4BIBkLVjN5fGRFos9rVIrPMjGsEPp+uZ705juOL3XFSgi92x0kJPfXZIwTkRFvamaTCxj/Vf0LtN5bjvpT04wDgWHWEjXNPy7Y4wNBZ7hMZuQhYm+Q+UG2PYSQCLbI1IzjH+BhtDsuECe1bTg1yPWKmrttTn1jhGsbctK7ump3j15EUjMQPUe02GFVRLWQF4Oa69mPnWyLJZUXbyMSTE0Xto8rqvsWSTjJp1rgf21w0WnbJwCcj9mS9qecYiypErY5RgUgE2iw2dcBMJce1F+sdbnf5sauJPo7c1hRBNVag2Kv4N7vjpARf7I6TEnyxO05K8MXuOCmhpwJdJ0RKYJCZPZZwcU1hho3/87Mf1Da/wwM7wiuHlA1NTbJxMqwDb/pP8M+/JUPsWXwnH1uaSKzjTCBDLZqGIHR0hrf36Z7RIk1c43PsX1EmiNdEhl+sP9ejNrfpFPTrYBXzKc6JoJoloyrPDM9yi5q6bHQywKvnJMb5m8M8w27pWv08+uQcjTnXbuDCXjSkhb441gLlusjea7f1tUqaRvWYWAjTTdnnC7qaTZZ0AJEsJd0l/lytlmav4t/sjpMSfLE7Tkrwxe44KcEXu+OkhJ4KdBZShKhkdPkimQ008iUjQqorBKERXZsoOcaj8+LVUWWDIheNxmZ0hl1jhAt9jXGtCLUGN+8R157W4ltxmn/+TjythaTcEpf6LGErd2qBb0iM6LhYRNDV9L1HU2dwhYQ/s2DZdPn1dzNa2JJaUhz0HGMhQPXdeLWy6fTzCLp2vxY+16ZET7S8Fr+yRkadLBNtlR+XEXQWkdhvJKsFy3zExWqrtLYl2m0V/2Z3nJTgi91xUoIvdsdJCT2uVKN9Djn+5sr1ar8vPcejWK6a1yV2uyUefBFF+nOMqqIs8fyisokGRZaZ4euOPs99/ZP3aH80MdoLZZf5nHJV7ZONvMCDJoovzSib5Czflon0cRLhV0d9OoAIeVEtxfCZqWToI0MD4jjaR0aW35NudgsluTP6Omq7uYayttOoCiRKdOdXjD7r/fydGRrUPnPTqELTqPHrL5V1tFRD9J4fKGibYzX+zuwrLyibwZhnxsne65eLf7M7Tkrwxe44KWHLi52IMkT0LBF9eWM8TESPEtGRjZ9b7CfiOM6V4PV8s38SwIsXjO8H8FgI4VoAj22MHcfZpmxJASCiKQA/D+A/AfjXG5vvBfC+jf9/CMA3AHzqUsdpdzM43eLBLp99/C42vubPdIDGgUUR2CLHAMKqKAOc02JLWOcCiAwOAYCuOE5oazFQ7TNg2DT156hoc4f8khaSCrN8js2rxpRNPMwFsszskrKRhLIW2rqDPKjHCs5p9xmZaENcfEuyRlmuzKXHANAY5vs1R/T96Ihy1xmj+nY8wO+1VRKscILf/NbQ1sSvrigTXl/Tfe5loE1S1hMoiIw2meEGbK3XmxTtZO+3SNZmZ7/bGn8A4LfAq7xPhBDOAcDGz/EtHstxnCvApoudiH4BwGwI4ek3cgIi+gQRHSSig2tLOvTTcZzesJW/Zd4D4MNE9CEABQADRPQnAGaIaDKEcI6IJgHMWjuHEB4E8CAATN1kBYw7jtMLNl3sIYRPA/g0ABDR+wD8mxDCR4nodwHcB+CBjZ8Pb3asudoAPvtN7qNPfY2v/+k7dHLIvg9Ps/HZP9bJECOfe4KNI+jjQCRjEBl/2Eg/vqv9+uV94rZ1tf8VrRtBPd1LjwFgfZLPe33YcHaJB8Nk13TZbJFTAQr6c1b62kneKG9c1tuaFb7NaDWOJC8SYXL6/N0yn2RU1j6qrADdLen5dERp7zoZfrW41x2jJPTVY/Nq2w/Xd7CxVRI76uPPv2P0kC/H/K9a1QYNwES2ysYZo+ROVVTBkT58eIvaPz0A4G4iOgLg7o2x4zjblNcVjxdC+AbOq+4IISwAeP+bPyXHcd4KPILOcVKCL3bHSQk9zXrLrANDP+CfL60yFyHu/OVn1X4fH3ucjT/zz7X3cPbYT/IN39DHgSFSSSgWAsgHfkrZVG+Wao/xmWnoJFK0kr3OAKAhAlZW9mubnIgpWh+zRDw+bKt6y0DU4kamiGYktHVEf/ZQ2Lx6Chl972UwilUGeQuPDHGWC3vhep3R1hJVgcKK7o9eG9TbsjlRPYaMYC3xHC2Bbkn0f7uhPK1sRkTE0Jm2jkBPxPdzXgh9ZNXR3sC/2R0nJfhid5yU4IvdcVJCb6vLDibofognbRQ+yyvDPH7iGrXbgfI5Nv7lse8qm8d/j/dA+qv/9W5lM3CS+5ZdozLKwjtEoMlVuupIqIuKow0j6aVs+LEdfuzWoNFuSexmBd7IarZW7/VuTuxYsA4k5h0b/l5iONIi0iVTMyr1VLivS0ZrpeEK962vqujqLS8vjbBxdVlX3OmISjFdY87xuKgCU9e+t9V6LJF6jOUSi/2abf1cl1s80Gcyq5OXGoHPqWt8F8s5yqq1l8K/2R0nJfhid5yU4IvdcVKCL3bHSQk9Feh2Fxbx+zd/gW37+EfuY+Pyk6KUM4Cj+ybYeNlIs/rw4DNsPPixdWXz/OpONn5lRbeIuirPBblqQ1d4mWlW2DhqGpVahoxy13V+u5v69MiIaXcN0Szs4UbWJ/aIKJW8ZAhbuUEekDFU1vdsZnFAbSPREikpGUE9oo95t6YFsU4/n/lQTref2jvIbZZX9HXIwJeuEazUbvJ7H+W0gLrWMiKIpABm6JXlQX7frLZNtSYP2BnIaOFX9l7fCpaoeDH8m91xUoIvdsdJCb7YHScl9NRnb4UYJ9vcUf0PP/0lPm78Q7XfN7/Ak1Hu/SffUjbH27wK643FM8rmyaWr2DhjBCQs1LlPWMxqP4oy3Gft5g2/umm0KBYJI52SdftFUI/RRmpY+OOJ4aOGLfhyAyXuN8aRDnwZ6Nd+tGxRvGj40W0xp4xRgbdS4r6ubFkMAIUM37ZjRFcWPnOMt1bqm9QlaFurvLqsVYG2k+hn1mnxZ0RG1eB6jfvj3VXt+9dEoM8fD9ypbO7f/RU2bhntn2QQTUZUk73UU/dvdsdJCb7YHScl+GJ3nJTgi91xUkJvs94MZGbP/7jr88rmE8VfY+OH/+RnlE3tIzwT7u7B55XN/j5eKvhUVWfYxUJ8Wzd6dlMkyyQbGWWGkKMqmvTpwI6oJUQiQ3FZrfMMqkEjGGZugQcnZfOG+BXzbda1Voo6+CMRlVgO7NA95E+tDLLxqtE2aaXBha3T9YqyWW3z/fKxUW66n4uoVlBNto+Xcu409LWurugAqviMEN+Maj5hjtvEbX1+mb34gyf1u1ffxY8jxTcAWO3y+1HJSAHVK9U4Turxxe44KcEXu+OkhJ767AGk2tW0RS/fl1s86QUAPnfnQ2z8z1q/rmz+5s9uZ+PSR3UTyRtLZ9n4UHFK2Zya5RU9K4O6Uqn0f1srOWUTr+jPURkgY7V6bovkEFm5BjCqpxjIOY4P6kCTtggiaSf6uJOlFbWt2uK+bWSU0+nP8/vf6uhXrVrlwTiLpyrKhkr8BsQFI1lE+MxN/QphbJhfx1yiE3ySup6jFXwjyS1zo/aAUUlXVPIdOqyP83/fzysk3zXwgrKpt/i1NkT537eq/ZPjOG8jfLE7Tkrwxe44KcEXu+OkBApb6a/zZp2MaA7ACQCjAHQz7O3P23HePufesF3mvDeEMGb9oqeL/bWTEh0MIdzW8xNfJm/Hefuce8PbYc7+Z7zjpARf7I6TEq7UYn/wCp33cnk7ztvn3Bu2/ZyviM/uOE7v8T/jHScl9HyxE9E9RPQSER0lovt7ff6tQESfJ6JZInr+gm3DRPQoER3Z+Dl0qWP0GiLaTURfJ6IXiegwEX1yY/u2nTcRFYjou0T0/Y05//bG9m0751chogwRPUtEX94Yb/s593SxE1EGwB8C+CCAGwF8hIhu7OUctsgfAbhHbLsfwGMhhGsBPLYx3k50APxmCOEAgDsA/MuNe7ud590EcFcI4Z0AbgFwDxHdge0951f5JIAXLxhv/zmHEHr2H4A7AXz1gvGnAXy6l3N4HXPdB+D5C8YvAZjc+P9JAC9d6TluMv+HAdz9dpk3gBKAZwC8a7vPGcAUzi/ouwB8+e3yfvT6z/hdAE5dMD69se3twEQI4RwAbPwcv8LzuShEtA/ArQCewjaf98afw4cAzAJ4NISw7ecM4A8A/BbA6kZt9zn3fLFbybb+zwFvIkTUB+CLAH4jhKCT0bcZIYQkhHALzn9b3k5EN1/hKV0SIvoFALMhhKev9FxeL71e7KcB7L5gPAXg7EVstxszRDQJABs/Z6/wfBRElMX5hf6nIYS/3Ni87ecNACGEKoBv4LxWsp3n/B4AHyai4wD+AsBdRPQn2N5zBtD7xf49ANcS0VVElAPwKwAe6fEc3iiPAHi1v/R9OO8TbxuIiAB8DsCLIYTfu+BX23beRDRGRJWN/y8C+ACAH2IbzzmE8OkQwlQIYR/Ov79/E0L4KLbxnF/jCogbHwLwIwAvA/h3V1q0uMgc/xzAOQBtnP9r5OMARnBelDmy8XP4Ss9TzPm9OO8SPQfg0MZ/H9rO8wbwEwCe3Zjz8wD+/cb2bTtnMf/34e8Eum0/Z4+gc5yU4BF0jpMSfLE7Tkrwxe44KcEXu+OkBF/sjpMSfLE7Tkrwxe44KcEXu+OkhP8PmAox8j99ufYAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[1,:,:,0])              #plot the first picture of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', verbose = 1, patience = 50)\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('bestmodel', monitor = 'val_accuracy', mode = 'max', verbose = 1, save_best_only= True )\n",
    "log_folder = '/logs'\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_folder, histogram_freq = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', strides=(1,1), input_shape=(48,48,1)),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same'), \n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1,1), padding ='valid'),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu', strides=(1,1), padding='same'),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(axis=3),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.6),\n",
    "    tf.keras.layers.Dense(7, activation='softmax')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 46, 46, 64)        640       \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 46, 46, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 46, 46, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 23, 23, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 23, 23, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 21, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 21, 21, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               524544    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 1799      \n",
      "=================================================================\n",
      "Total params: 1,045,895\n",
      "Trainable params: 1,044,743\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=adam, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "y_train=tf.keras.utils.to_categorical(Y_train, num_classes = 7)\n",
    "y_test=tf.keras.utils.to_categorical(Y_test, num_classes = 7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1770/1770 [==============================] - 69s 39ms/step - loss: 1.4109 - accuracy: 0.4600 - val_loss: 1.1687 - val_accuracy: 0.5573\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.55729, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 2/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 1.1990 - accuracy: 0.5412 - val_loss: 1.0264 - val_accuracy: 0.6081\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.55729 to 0.60814, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 3/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 1.0583 - accuracy: 0.5988 - val_loss: 0.9187 - val_accuracy: 0.6488\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.60814 to 0.64882, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 4/200\n",
      "1770/1770 [==============================] - 66s 38ms/step - loss: 0.9520 - accuracy: 0.6374 - val_loss: 0.8725 - val_accuracy: 0.6693\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.64882 to 0.66932, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 5/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.8540 - accuracy: 0.6749 - val_loss: 0.8342 - val_accuracy: 0.6852\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.66932 to 0.68521, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 6/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.7749 - accuracy: 0.7105 - val_loss: 0.7713 - val_accuracy: 0.7164\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.68521 to 0.71635, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 7/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.6913 - accuracy: 0.7443 - val_loss: 0.7186 - val_accuracy: 0.7421\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.71635 to 0.74209, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 8/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.6132 - accuracy: 0.7732 - val_loss: 0.7041 - val_accuracy: 0.7459\n",
      "\n",
      "Epoch 00008: val_accuracy improved from 0.74209 to 0.74591, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 9/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.5439 - accuracy: 0.7991 - val_loss: 0.6565 - val_accuracy: 0.7729\n",
      "\n",
      "Epoch 00009: val_accuracy improved from 0.74591 to 0.77292, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 10/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.4827 - accuracy: 0.8235 - val_loss: 0.6475 - val_accuracy: 0.7823\n",
      "\n",
      "Epoch 00010: val_accuracy improved from 0.77292 to 0.78230, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 11/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.4214 - accuracy: 0.8477 - val_loss: 0.6150 - val_accuracy: 0.8018\n",
      "\n",
      "Epoch 00011: val_accuracy improved from 0.78230 to 0.80184, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 12/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.3736 - accuracy: 0.8650 - val_loss: 0.6072 - val_accuracy: 0.8066\n",
      "\n",
      "Epoch 00012: val_accuracy improved from 0.80184 to 0.80661, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 13/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.3374 - accuracy: 0.8775 - val_loss: 0.5916 - val_accuracy: 0.8181\n",
      "\n",
      "Epoch 00013: val_accuracy improved from 0.80661 to 0.81805, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 14/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.2971 - accuracy: 0.8926 - val_loss: 0.6070 - val_accuracy: 0.8252\n",
      "\n",
      "Epoch 00014: val_accuracy improved from 0.81805 to 0.82520, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 15/200\n",
      "1770/1770 [==============================] - 66s 38ms/step - loss: 0.2621 - accuracy: 0.9057 - val_loss: 0.5882 - val_accuracy: 0.8312\n",
      "\n",
      "Epoch 00015: val_accuracy improved from 0.82520 to 0.83124, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 16/200\n",
      "1770/1770 [==============================] - 68s 38ms/step - loss: 0.2400 - accuracy: 0.9141 - val_loss: 0.5934 - val_accuracy: 0.8365\n",
      "\n",
      "Epoch 00016: val_accuracy improved from 0.83124 to 0.83648, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 17/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.2113 - accuracy: 0.9252 - val_loss: 0.6946 - val_accuracy: 0.8258\n",
      "\n",
      "Epoch 00017: val_accuracy did not improve from 0.83648\n",
      "Epoch 18/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1955 - accuracy: 0.9313 - val_loss: 0.6407 - val_accuracy: 0.8433\n",
      "\n",
      "Epoch 00018: val_accuracy improved from 0.83648 to 0.84332, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 19/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1780 - accuracy: 0.9380 - val_loss: 0.6854 - val_accuracy: 0.8351\n",
      "\n",
      "Epoch 00019: val_accuracy did not improve from 0.84332\n",
      "Epoch 20/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1662 - accuracy: 0.9412 - val_loss: 0.7270 - val_accuracy: 0.8324\n",
      "\n",
      "Epoch 00020: val_accuracy did not improve from 0.84332\n",
      "Epoch 21/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1545 - accuracy: 0.9461 - val_loss: 0.7455 - val_accuracy: 0.8443\n",
      "\n",
      "Epoch 00021: val_accuracy improved from 0.84332 to 0.84427, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 22/200\n",
      "1770/1770 [==============================] - 66s 38ms/step - loss: 0.1396 - accuracy: 0.9511 - val_loss: 0.7367 - val_accuracy: 0.8378\n",
      "\n",
      "Epoch 00022: val_accuracy did not improve from 0.84427\n",
      "Epoch 23/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.1331 - accuracy: 0.9538 - val_loss: 0.7281 - val_accuracy: 0.8436\n",
      "\n",
      "Epoch 00023: val_accuracy did not improve from 0.84427\n",
      "Epoch 24/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1267 - accuracy: 0.9566 - val_loss: 0.7478 - val_accuracy: 0.8471\n",
      "\n",
      "Epoch 00024: val_accuracy improved from 0.84427 to 0.84713, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 25/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1217 - accuracy: 0.9582 - val_loss: 0.7764 - val_accuracy: 0.8440\n",
      "\n",
      "Epoch 00025: val_accuracy did not improve from 0.84713\n",
      "Epoch 26/200\n",
      "1770/1770 [==============================] - 68s 39ms/step - loss: 0.1108 - accuracy: 0.9616 - val_loss: 0.7888 - val_accuracy: 0.8411\n",
      "\n",
      "Epoch 00026: val_accuracy did not improve from 0.84713\n",
      "Epoch 27/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.1108 - accuracy: 0.9617 - val_loss: 0.8423 - val_accuracy: 0.8416\n",
      "\n",
      "Epoch 00027: val_accuracy did not improve from 0.84713\n",
      "Epoch 28/200\n",
      "1770/1770 [==============================] - 68s 38ms/step - loss: 0.1031 - accuracy: 0.9641 - val_loss: 0.8594 - val_accuracy: 0.8413\n",
      "\n",
      "Epoch 00028: val_accuracy did not improve from 0.84713\n",
      "Epoch 29/200\n",
      "1770/1770 [==============================] - 68s 38ms/step - loss: 0.1028 - accuracy: 0.9650 - val_loss: 0.8573 - val_accuracy: 0.8414\n",
      "\n",
      "Epoch 00029: val_accuracy did not improve from 0.84713\n",
      "Epoch 30/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0931 - accuracy: 0.9678 - val_loss: 0.8591 - val_accuracy: 0.8451\n",
      "\n",
      "Epoch 00030: val_accuracy did not improve from 0.84713\n",
      "Epoch 31/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0940 - accuracy: 0.9675 - val_loss: 0.8734 - val_accuracy: 0.8397\n",
      "\n",
      "Epoch 00031: val_accuracy did not improve from 0.84713\n",
      "Epoch 32/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0917 - accuracy: 0.9689 - val_loss: 0.9126 - val_accuracy: 0.8384\n",
      "\n",
      "Epoch 00032: val_accuracy did not improve from 0.84713\n",
      "Epoch 33/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0880 - accuracy: 0.9701 - val_loss: 0.8982 - val_accuracy: 0.8446\n",
      "\n",
      "Epoch 00033: val_accuracy did not improve from 0.84713\n",
      "Epoch 34/200\n",
      "1770/1770 [==============================] - 68s 38ms/step - loss: 0.0856 - accuracy: 0.9711 - val_loss: 0.8836 - val_accuracy: 0.8474\n",
      "\n",
      "Epoch 00034: val_accuracy improved from 0.84713 to 0.84745, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 35/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0782 - accuracy: 0.9735 - val_loss: 0.8707 - val_accuracy: 0.8538\n",
      "\n",
      "Epoch 00035: val_accuracy improved from 0.84745 to 0.85381, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 36/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0782 - accuracy: 0.9734 - val_loss: 0.9306 - val_accuracy: 0.8521\n",
      "\n",
      "Epoch 00036: val_accuracy did not improve from 0.85381\n",
      "Epoch 37/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0787 - accuracy: 0.9734 - val_loss: 0.9122 - val_accuracy: 0.8474\n",
      "\n",
      "Epoch 00037: val_accuracy did not improve from 0.85381\n",
      "Epoch 38/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0756 - accuracy: 0.9739 - val_loss: 0.9420 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00038: val_accuracy did not improve from 0.85381\n",
      "Epoch 39/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0731 - accuracy: 0.9747 - val_loss: 0.9239 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00039: val_accuracy did not improve from 0.85381\n",
      "Epoch 40/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0733 - accuracy: 0.9746 - val_loss: 0.9316 - val_accuracy: 0.8525\n",
      "\n",
      "Epoch 00040: val_accuracy did not improve from 0.85381\n",
      "Epoch 41/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0705 - accuracy: 0.9763 - val_loss: 1.0006 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00041: val_accuracy did not improve from 0.85381\n",
      "Epoch 42/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0682 - accuracy: 0.9768 - val_loss: 1.0004 - val_accuracy: 0.8497\n",
      "\n",
      "Epoch 00042: val_accuracy did not improve from 0.85381\n",
      "Epoch 43/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0671 - accuracy: 0.9775 - val_loss: 0.9712 - val_accuracy: 0.8441\n",
      "\n",
      "Epoch 00043: val_accuracy did not improve from 0.85381\n",
      "Epoch 44/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0682 - accuracy: 0.9764 - val_loss: 1.0244 - val_accuracy: 0.8530TA: 1s - l\n",
      "\n",
      "Epoch 00044: val_accuracy did not improve from 0.85381\n",
      "Epoch 45/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0594 - accuracy: 0.9801 - val_loss: 1.0301 - val_accuracy: 0.8506\n",
      "\n",
      "Epoch 00045: val_accuracy did not improve from 0.85381\n",
      "Epoch 46/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0613 - accuracy: 0.9793 - val_loss: 1.0518 - val_accuracy: 0.8430\n",
      "\n",
      "Epoch 00046: val_accuracy did not improve from 0.85381\n",
      "Epoch 47/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0644 - accuracy: 0.9783 - val_loss: 1.0159 - val_accuracy: 0.8482\n",
      "\n",
      "Epoch 00047: val_accuracy did not improve from 0.85381\n",
      "Epoch 48/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0582 - accuracy: 0.9800 - val_loss: 1.0625 - val_accuracy: 0.8532\n",
      "\n",
      "Epoch 00048: val_accuracy did not improve from 0.85381\n",
      "Epoch 49/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0622 - accuracy: 0.9790 - val_loss: 1.0865 - val_accuracy: 0.8494\n",
      "\n",
      "Epoch 00049: val_accuracy did not improve from 0.85381\n",
      "Epoch 50/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0569 - accuracy: 0.9801 - val_loss: 1.0536 - val_accuracy: 0.8505\n",
      "\n",
      "Epoch 00050: val_accuracy did not improve from 0.85381\n",
      "Epoch 51/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0556 - accuracy: 0.9816 - val_loss: 1.0898 - val_accuracy: 0.8571\n",
      "\n",
      "Epoch 00051: val_accuracy improved from 0.85381 to 0.85714, saving model to bestmodel\n",
      "INFO:tensorflow:Assets written to: bestmodel\\assets\n",
      "Epoch 52/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0562 - accuracy: 0.9806 - val_loss: 1.0259 - val_accuracy: 0.8519\n",
      "\n",
      "Epoch 00052: val_accuracy did not improve from 0.85714\n",
      "Epoch 53/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0542 - accuracy: 0.9822 - val_loss: 1.0955 - val_accuracy: 0.8511\n",
      "\n",
      "Epoch 00053: val_accuracy did not improve from 0.85714\n",
      "Epoch 54/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0509 - accuracy: 0.9826 - val_loss: 1.1654 - val_accuracy: 0.8462\n",
      "\n",
      "Epoch 00054: val_accuracy did not improve from 0.85714\n",
      "Epoch 55/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0538 - accuracy: 0.9815 - val_loss: 1.1029 - val_accuracy: 0.8514\n",
      "\n",
      "Epoch 00055: val_accuracy did not improve from 0.85714\n",
      "Epoch 56/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0531 - accuracy: 0.9824 - val_loss: 1.0806 - val_accuracy: 0.8503\n",
      "\n",
      "Epoch 00056: val_accuracy did not improve from 0.85714\n",
      "Epoch 57/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0525 - accuracy: 0.9826 - val_loss: 1.1211 - val_accuracy: 0.8503\n",
      "\n",
      "Epoch 00057: val_accuracy did not improve from 0.85714\n",
      "Epoch 58/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0489 - accuracy: 0.9830 - val_loss: 1.1283 - val_accuracy: 0.8524\n",
      "\n",
      "Epoch 00058: val_accuracy did not improve from 0.85714\n",
      "Epoch 59/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0531 - accuracy: 0.9823 - val_loss: 1.0677 - val_accuracy: 0.8530\n",
      "\n",
      "Epoch 00059: val_accuracy did not improve from 0.85714\n",
      "Epoch 60/200\n",
      "1770/1770 [==============================] - 66s 37ms/step - loss: 0.0474 - accuracy: 0.9841 - val_loss: 1.1410 - val_accuracy: 0.8494\n",
      "\n",
      "Epoch 00060: val_accuracy did not improve from 0.85714\n",
      "Epoch 61/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0482 - accuracy: 0.9836 - val_loss: 1.1584 - val_accuracy: 0.8524\n",
      "\n",
      "Epoch 00061: val_accuracy did not improve from 0.85714\n",
      "Epoch 62/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0465 - accuracy: 0.9840 - val_loss: 1.1102 - val_accuracy: 0.8557\n",
      "\n",
      "Epoch 00062: val_accuracy did not improve from 0.85714\n",
      "Epoch 63/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0462 - accuracy: 0.9841 - val_loss: 1.1421 - val_accuracy: 0.8498\n",
      "\n",
      "Epoch 00063: val_accuracy did not improve from 0.85714\n",
      "Epoch 64/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0471 - accuracy: 0.9838 - val_loss: 1.0954 - val_accuracy: 0.8535\n",
      "\n",
      "Epoch 00064: val_accuracy did not improve from 0.85714\n",
      "Epoch 65/200\n",
      "1770/1770 [==============================] - 67s 38ms/step - loss: 0.0454 - accuracy: 0.9847 - val_loss: 1.1538 - val_accuracy: 0.8527\n",
      "\n",
      "Epoch 00065: val_accuracy did not improve from 0.85714\n",
      "Epoch 00065: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2185ea02220>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=200, validation_data=(X_test, y_test), callbacks =[model_checkpoint,early_stopping,tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=tf.keras.models.load_model('modelfinal')\n",
    "#preds=model.predict(X_train)\n",
    "#def get_class(preds):\n",
    "#    pred_class=np.zeros((preds.shape[0],1))\n",
    "    \n",
    "#    for i in range(len(preds)):\n",
    " #       pred_class[i] = np.argmax(preds[i])\n",
    "\n",
    " #   return pred_class\n",
    "#pred_class_train=get_class(preds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted result is: Angry\n",
      "[[9.9637860e-01 8.0642199e-14 3.3487491e-03 6.6112091e-16 2.7186517e-04\n",
      "  1.3191911e-16 8.6216943e-07]]\n"
     ]
    }
   ],
   "source": [
    "label_dict = {0 : 'Angry', 1 : 'Disgust', 2 : 'Fear', 3 : 'Happiness', 4 : 'Sad', 5 : 'Surprise', 6 : 'Neutral'}\n",
    "img_path = 'C:/Users/vasil/Desktop/tensorflow/FER/code/AI/src/jupyter/pictures/cc2.jpeg'\n",
    "img= image.load_img(img_path, color_mode='grayscale', target_size=(48,48))\n",
    "x= image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "prediction = np.argmax(model.predict(x))\n",
    "print('The predicted result is: ' + label_dict[prediction])\n",
    "print(model.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#python -m tensorboard.main --logdir=logs/ insert this in terminal for magic"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8a09ddbbea9b09bd8fe9d9c3d3756891efc55aae894e87570a5b525b82263952"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('ai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
